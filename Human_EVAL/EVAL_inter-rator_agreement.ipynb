{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_6models = [\n",
    "    \"gold\",\n",
    "    \"chatgpt\",\n",
    "    \"flan-t5-xxl\",\n",
    "    \"alpaca-7B\",\n",
    "    \"alpaca-13B\",\n",
    "    \"dolly-v2-7B\",\n",
    "    \"dolly-v2-12B\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3models = [\n",
    "    \"gold\",\n",
    "    \"chatgpt\",\n",
    "    \"flan-t5-xxl\",\n",
    "    \"alpaca-7B\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In main paper, top 3 models\n",
    "# In appendix, all 6 models\n",
    "if_top3_models = True\n",
    "\n",
    "if if_top3_models:\n",
    "    mode = \"TOP 3 models\"\n",
    "else:\n",
    "    mode = \"ALL 6 models\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter-annotator stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "606"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "from scipy import stats\n",
    "from statistics import mean\n",
    "\n",
    "file_name_1 = \"TURKERS-EVAL_data-intersection-ALL_6_models.csv\"\n",
    "file_name_2 = \"TURKERS-EVAL_data-intersection-TOP_3_models.csv\"\n",
    "\n",
    "if if_top3_models == True:\n",
    "    eval_df = pd.concat([\n",
    "        pd.read_csv(file_name_1),\n",
    "        pd.read_csv(file_name_2),\n",
    "    ])\n",
    "    eval_df = eval_df[eval_df[\"Model\"].isin(top_3models)]\n",
    "else:\n",
    "    eval_df = pd.read_csv(file_name_1)\n",
    "\n",
    "len(eval_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fleiss Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fleiss_kappa(df: pd.DataFrame, feature, len_annotators): # df == annotations within each Group\n",
    "    \"\"\"\n",
    "    - Fleiss Kappa:\n",
    "    :input: raw_task_data = data containing category assignment with subjects in rows and raters in columns.\n",
    "    :use: for questions 1, 3 & 4\n",
    "    \"\"\"\n",
    "    f_task_data = list()\n",
    "\n",
    "    hid_ids = np.unique(list(df[\"HIT ID\"]))\n",
    "\n",
    "    for hid_id in hid_ids:\n",
    "        same_hit_df = df[df[\"HIT ID\"] == hid_id] # annotations within each hit id\n",
    "\n",
    "        if len(same_hit_df) == len_annotators: # only calculate annotations == len_annotators (2)\n",
    "            anno_vec = [] # anno_vectors for each dimension, e.g.: [yes, no, yes]\n",
    "\n",
    "            for _, row in same_hit_df.iterrows():\n",
    "                anno_vec.append (row[feature])\n",
    "\n",
    "            f_task_data.append(anno_vec)\n",
    "\n",
    "    #print (f_task_data)\n",
    "\n",
    "    agg = irr.aggregate_raters(f_task_data) # returns a tuple (data, categories)\n",
    "    #print (agg)\n",
    "    fleiss_rslt = irr.fleiss_kappa(agg[0], method='randolph')\n",
    "\n",
    "    return round (fleiss_rslt, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krippendorff's alpha (ordinal distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_NA(x):\n",
    "    \"\"\"\n",
    "    :Usage: to calculate the mean of the 3 annotators' scores\n",
    "    - Returns the mean of a list of 3 annotators:\n",
    "        - If >= 2 annotators put \"NA\", returns \"NA\"\n",
    "        - else: returns the mean of the 3 annotators, ignoring \"NA\"\n",
    "    \"\"\"\n",
    "    if x.isna().sum() >= 2:\n",
    "        return np.nan\n",
    "    else:\n",
    "        non_nan_values = [i for i in x if not np.isnan(i)]\n",
    "        return sum(non_nan_values) / len(non_nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.agreement import AnnotationTask\n",
    "from nltk.metrics.distance import interval_distance\n",
    "\n",
    "def krippendorff_alpha(df: pd.DataFrame, feature, show_per_worker=False):\n",
    "    \"\"\"\n",
    "    - Krippendorff's Alpha using Ordinal Distance:\n",
    "    - Apply on the human evaluation dataframe *after* converting the selections to numerical values (eval_df_copy).\n",
    "    :usage: do it on each worker against all others (mean), then take average of all workers\n",
    "    :input: df - data containing category assignment with subjects in rows and raters in columns.\n",
    "            feature - the column name of the feature to calculate alpha for.\n",
    "    :return: Krippendorff's alpha using ordinal distance.\n",
    "    \"\"\"\n",
    "    agg_dict = {}\n",
    "    agg_dict[feature] = mean_NA\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    worker_ids = np.unique(list(df[\"Worker ID\"]))\n",
    "    for worker_id in worker_ids:\n",
    "        worker_df = df[df[\"Worker ID\"] == worker_id] # annotations by the single annotator\n",
    "        worker_df = worker_df.groupby(\"HIT ID\").agg(agg_dict).reset_index(drop=False).sort_values(by=\"HIT ID\") # sort by HIT ID\n",
    "        all_other_worker_df = df[ (df[\"Worker ID\"] != worker_id) & (df[\"HIT ID\"].isin(worker_df[\"HIT ID\"])) ] # annotations by all other annotators on the same posts\n",
    "        all_other_worker_mean_df = all_other_worker_df.groupby(\"HIT ID\").agg(agg_dict).reset_index(drop=False) # taking the mean of the other annotators' scores\n",
    "        all_other_worker_mean_df = all_other_worker_mean_df.sort_values(by=\"HIT ID\") # sort by HIT ID\n",
    "\n",
    "        # filter out HITs that have only 1 annotator\n",
    "        worker_df = worker_df[worker_df[\"HIT ID\"].isin(all_other_worker_mean_df[\"HIT ID\"])]\n",
    "        assert worker_df[\"HIT ID\"].tolist() == all_other_worker_mean_df[\"HIT ID\"].tolist() # make sure the two dataframes are aligned\n",
    "\n",
    "        task_data = []\n",
    "        for idx, hit_id in enumerate(np.unique(list(worker_df[\"HIT ID\"]))):\n",
    "            task_data.append ( (\"coder_1\", idx, worker_df[worker_df[\"HIT ID\"] == hit_id][feature].tolist()[0]) )\n",
    "            task_data.append ( (\"coder_2\", idx, all_other_worker_mean_df[all_other_worker_mean_df[\"HIT ID\"] == hit_id][feature].tolist()[0]) )\n",
    "\n",
    "        task = AnnotationTask(distance = interval_distance)\n",
    "        task.load_array(task_data)\n",
    "        worker_krippendorff = task.alpha()\n",
    "        result.append(worker_krippendorff)\n",
    "        \n",
    "        if show_per_worker:\n",
    "            print (worker_id, worker_krippendorff)\n",
    "\n",
    "    return round (mean(result), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_corr(df: pd.DataFrame, feature, method, show_per_worker=False): # df == annotations within each Group\n",
    "    \"\"\"\n",
    "    - Calculate the pair-wise Pearson correlation coefficient between each pair of annotators on each dimension\n",
    "    - We do it on each dimension so that we can average --> cross examples\n",
    "    :usage: Compare each annotator's work against the average of the other one/two annotators\n",
    "    \"\"\"\n",
    "    agg_dict = {}\n",
    "    agg_dict[feature] = mean_NA\n",
    "\n",
    "    results = []\n",
    "\n",
    "    worker_ids = np.unique(list(df[\"Worker ID\"]))\n",
    "\n",
    "    # cross-posts, for each dimension\n",
    "    for worker_id in worker_ids:\n",
    "        worker_results = []\n",
    "\n",
    "        worker_df = df[df[\"Worker ID\"] == worker_id] # annotations by the single annotator\n",
    "        worker_df = worker_df.groupby(\"HIT ID\").agg(agg_dict).reset_index(drop=False).sort_values(by=\"HIT ID\") # sort by HIT ID\n",
    "        all_other_worker_df = df[ (df[\"Worker ID\"] != worker_id) & (df[\"HIT ID\"].isin(worker_df[\"HIT ID\"])) ] # annotations by all other annotators on the same posts\n",
    "        all_other_worker_mean_df = all_other_worker_df.groupby(\"HIT ID\").agg(agg_dict).reset_index(drop=False) # taking the mean of the other annotators' scores\n",
    "        all_other_worker_mean_df = all_other_worker_mean_df.sort_values(by=\"HIT ID\") # sort by HIT ID\n",
    "\n",
    "        # filter out HITs that have only 1 annotator\n",
    "        worker_df = worker_df[worker_df[\"HIT ID\"].isin(all_other_worker_mean_df[\"HIT ID\"])]\n",
    "\n",
    "        #display (worker_df)\n",
    "        #display (all_other_worker_mean_df)\n",
    "        assert worker_df[\"HIT ID\"].tolist() == all_other_worker_mean_df[\"HIT ID\"].tolist() # make sure the two dataframes are aligned\n",
    "\n",
    "        # annotations from a single annotator compared to other annotators' mean scores, in each feature asked\n",
    "        annotator_vec = worker_df[feature].tolist() # annotations from a single annotator, in each dimension for all posts that he annotated\n",
    "        other_annotators_mean_vec = all_other_worker_mean_df[feature].tolist() # mean scores from other annotators, in each dimension for all posts that they annotated\n",
    "\n",
    "        #print (annotator_vec)\n",
    "        #print (other_annotators_mean_vec)\n",
    "\n",
    "        if len(annotator_vec) >= 2: # ValueError: x and y must have length at least 2.\n",
    "            if np.var(annotator_vec) != 0 and np.var(other_annotators_mean_vec) != 0: # non-0 variance\n",
    "                if method == \"pearson\":\n",
    "                    results.append (stats.pearsonr(annotator_vec, other_annotators_mean_vec))\n",
    "                    worker_results.append (stats.pearsonr(annotator_vec, other_annotators_mean_vec))\n",
    "                elif method == \"spearman\":\n",
    "                    results.append (stats.spearmanr(annotator_vec, other_annotators_mean_vec))\n",
    "                    worker_results.append (stats.spearmanr(annotator_vec, other_annotators_mean_vec))\n",
    "            else:\n",
    "                print (\"Error: 0 variance!\")\n",
    "        else:\n",
    "            print (\"Error: x and y must have length at least 2.\")\n",
    "\n",
    "        #print (stats.pearsonr(annotator_vec, other_annotators_mean_vec))\n",
    "\n",
    "        if show_per_worker and worker_results != []:\n",
    "            print (f\"Worker {worker_id}: corr = {round (mean([item[0] for item in worker_results]), 3)}, p = {round (mean([item[1] for item in worker_results]), 3)}\")\n",
    "\n",
    "    if results == []:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return round (mean([item[0] for item in results]), 3), round (mean([item[1] for item in results]), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the string values to integers\n",
    "mapping_dict = {\n",
    "    \"yes\": 1,\n",
    "    \"minor\": 0.5,\n",
    "    \"maybe\": 0.5,\n",
    "    \"no\": 0,\n",
    "}\n",
    "\n",
    "mapping_dict_rel = {\n",
    "    1:  0,\n",
    "    2:  0.25,\n",
    "    3:  0.5,\n",
    "    4:  0.75,\n",
    "    5:  1\n",
    "    }\n",
    "\n",
    "eval_df_copy = eval_df.copy()\n",
    "eval_df_copy[[\"factuality\", \"justifies\", \"usefulness\"]] = eval_df_copy[[\"factuality\", \"justifies\", \"usefulness\"]].apply(lambda x: x.map(mapping_dict))\n",
    "eval_df_copy[[\"relevance\"]] = eval_df_copy[[\"relevance\"]].apply(lambda x: x.map(mapping_dict_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_df) - len(np.unique(eval_df[\"HIT ID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 3 models\n",
      "hits: 327\n",
      "================================================\n",
      "              *** Fleiss Kappa ***\n",
      "Factuality: 0.29\n",
      "Justifies: 0.699\n",
      "Relevance: 0.355\n",
      "Usefulness: 0.446\n",
      "------------------------------------------------\n",
      "*** Krippendorff's alpha (interval distance) ***\n",
      "Factuality: 0.59\n",
      "Justifies: 0.576\n",
      "Relevance: 0.718\n",
      "Usefulness: 0.668\n",
      "------------------------------------------------\n",
      "         *** Pearson Correlation ***\n",
      "Worker A1KEA2Z47S3UPI: corr = 0.659, p = 0.0\n",
      "Worker A2A6FH0F7LD9ND: corr = 0.8, p = 0.0\n",
      "Worker A2MO3EE6D0P3KR: corr = 0.97, p = 0.006\n",
      "Worker AKA8TN8H8DQ6T: corr = 0.671, p = 0.0\n",
      "Pearson Correlation on RELEVANCE: (0.775, 0.002)\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "print (mode)\n",
    "print (\"hits:\", len(np.unique(eval_df[\"HIT ID\"])))\n",
    "print (\"================================================\")\n",
    "print (\"              *** Fleiss Kappa ***\")\n",
    "print (\"Factuality:\", fleiss_kappa(eval_df, \"factuality\", 2))\n",
    "print (\"Justifies:\", fleiss_kappa(eval_df, \"justifies\", 2))\n",
    "print (\"Relevance:\", fleiss_kappa(eval_df, \"relevance\", 2))\n",
    "print (\"Usefulness:\", fleiss_kappa(eval_df, \"usefulness\", 2))\n",
    "print (\"------------------------------------------------\")\n",
    "print (\"*** Krippendorff's alpha (interval distance) ***\")\n",
    "print (\"Factuality:\", krippendorff_alpha(eval_df_copy, \"factuality\"))\n",
    "print (\"Justifies:\", krippendorff_alpha(eval_df_copy, \"justifies\"))\n",
    "print (\"Relevance:\", krippendorff_alpha(eval_df_copy, \"relevance\"))\n",
    "print (\"Usefulness:\", krippendorff_alpha(eval_df_copy, \"usefulness\"))\n",
    "print (\"------------------------------------------------\")\n",
    "print (\"         *** Pearson Correlation ***\")\n",
    "print (\"Pearson Correlation on RELEVANCE:\", calculate_corr(eval_df, \"relevance\", \"pearson\", show_per_worker=True))\n",
    "print (\"================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
